{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy4F0v2wg7YIHaT/25iQjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imeldp96/qsar_study/blob/main/Support_Vector_Regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation"
      ],
      "metadata": {
        "id": "DbMHvZENnWbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv2ZyGw6lDBB"
      },
      "outputs": [],
      "source": [
        "!pip install kennard_stone #to install kennard-stone splitting algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "fAvSFp9RmiT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to import dependencies\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas import DataFrame as df\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score, mean_absolute_percentage_error\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#import kennard_stone as ks #activate this line to install ks as well (after installing), deactivate if not needed\n",
        "\n",
        "random = 42 #random number for consistent results, can be changed to anything"
      ],
      "metadata": {
        "id": "4fEeBrgtmmOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/data/all.csv', index_col=[0], header=[0]) #change accordingly with the location of your csv file"
      ],
      "metadata": {
        "id": "kFtzv6Oim0fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X= data.drop('pIC50', axis=1) #Select the descriptor's columns\n",
        "\n",
        "#X=data[['X3', 'X21','X11']] #vi-qc\n",
        "#X=data[['X11', 'X15', 'X16']] #ga-qc\n",
        "\n",
        "y = data['pIC50']  # Setting y as the target variable.\n",
        "X.head()"
      ],
      "metadata": {
        "id": "D5O7Oi8nm2g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kennard-Stone Algorithm for training-test set division\n",
        "#X_train, X_test, y_train, y_test = ks.train_test_split(X, y, test_size=0.2) #change test_size accordingly with the proportion of training-test set"
      ],
      "metadata": {
        "id": "uGuhJy3mm-Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dont run this if you run the KS set division\n",
        "#only run this if the training and test set has been divided into different rows\n",
        "\n",
        "X_train = X.iloc[:19] #select rows for train set\n",
        "X_test = X.iloc[19:28] #select rows for test set\n",
        "\n",
        "y_train = y.iloc[:19]\n",
        "y_test = y.iloc[19:28]"
      ],
      "metadata": {
        "id": "EiPaO8YhnEMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "id": "jABVnqJPnTF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardization of X values\n",
        "#svr is very sensitive to scale of the features/descriptors, thus standardization is needed\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to your training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled)"
      ],
      "metadata": {
        "id": "AUyzK7fMnIg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVR Model Building"
      ],
      "metadata": {
        "id": "A7pEbxPwnabO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting\n",
        "svr=svm.SVR()\n",
        "svr_fit = svr.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "3JGFSFqwnfqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction on the training dataset\n",
        "ytrain_pred = svr_fit.predict(X_train_scaled)\n",
        "#training model evaluation\n",
        "\n",
        "#training r-sq\n",
        "print('The training r_sq is: %.3f'% svr_fit.score(X_train_scaled, y_train))\n",
        "\n",
        "#RMSE\n",
        "print('The RMSE is: %.3f'% np.sqrt(mean_squared_error(y_train, ytrain_pred)))\n",
        "\n",
        "#MAPE\n",
        "print('The MAPE is: %.3f'% mean_absolute_percentage_error(y_train, ytrain_pred))"
      ],
      "metadata": {
        "id": "rlvqHmdJniRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction on the testing data\n",
        "ytest_pred = svr_fit.predict(X_test_scaled)\n",
        "\n",
        "#testing coef. of determination\n",
        "print('The testing r_sq is: %.3f'% r2_score(y_test, ytest_pred))\n",
        "#model evaluation metrics on test set\n",
        "\n",
        "#RMSE\n",
        "print('The RMSE is: %.3f'% np.sqrt(mean_squared_error(y_test, ytest_pred)))\n",
        "\n",
        "#MAPE\n",
        "print('The MAPE is: %.3f'% mean_absolute_percentage_error(y_test, ytest_pred))"
      ],
      "metadata": {
        "id": "sWEbQbPxnnGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the predicted targets\n",
        "df_ytrain = pd.DataFrame(y_train)\n",
        "df_ytrainpred = pd.DataFrame(ytrain_pred)\n",
        "df_ytest = pd.DataFrame(y_test)\n",
        "df_ytestpred = pd.DataFrame(ytest_pred)"
      ],
      "metadata": {
        "id": "kdEZW3Mmnp0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make pred vs exp graph\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_scatter_with_line(y_train, y_train_pred, y_test, y_test_pred):\n",
        "    fig = plt.figure(figsize=(8, 8))  # Set the figure size\n",
        "    plt.scatter(y_train, y_train_pred, color='blue', label='Training Set', alpha=0.5, s=70)\n",
        "    plt.scatter(y_test, y_test_pred, color='red', label='Test Set', alpha=0.5, s=50)\n",
        "    plt.plot([np.min(np.concatenate([y_train, y_test])), np.max(np.concatenate([y_train, y_test]))],\n",
        "             [np.min(np.concatenate([y_train, y_test])), np.max(np.concatenate([y_train, y_test]))],\n",
        "             color='black', linestyle='--')  # Diagonal line\n",
        "    plt.title('Experimental vs Predicted LogKow', fontsize=16)\n",
        "    plt.xlabel('Experimental LogKow', fontsize=14)\n",
        "    plt.ylabel('Predicted LogKow', fontsize=14)\n",
        "    plt.axis('square')  # Set aspect ratio to be equal\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Set face color of the figure to white\n",
        "    fig.patch.set_facecolor('none')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have y_train, ytrain_pred, y_test, and ytest_pred as arrays\n",
        "plot_scatter_with_line(y_train, ytrain_pred, y_test, ytest_pred)"
      ],
      "metadata": {
        "id": "Gpm6yoAvnr0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters Tuning"
      ],
      "metadata": {
        "id": "jaYPiPMtn0lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the parameter grid based on the new specifications\n",
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'epsilon': np.arange(0.1, 1.01, 0.01).tolist(),\n",
        "        'C': np.arange(1, 501, 1).tolist()\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'epsilon': np.arange(0.1, 1.01, 0.01).tolist(),\n",
        "        'C': np.arange(1, 501, 1).tolist(),\n",
        "        'degree': np.arange(1, 21, 1).tolist()\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create a base model\n",
        "svr = svm.SVR()\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svr,\n",
        "    param_grid=param_grid,\n",
        "    cv=8,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring=['r2', 'neg_root_mean_squared_error'],\n",
        "    refit='neg_root_mean_squared_error',\n",
        "    error_score='raise'\n",
        ")\n"
      ],
      "metadata": {
        "id": "qsWYqTZ-n25_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "Ei8oAZEJoaU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the randomized hyperparameters and their values\n",
        "grids_params = grid_search.cv_results_\n",
        "\n",
        "# Convert the randomized hyperparameters to a DataFrame\n",
        "hyperparameters_df = pd.DataFrame(grids_params)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "hyperparameters_df.to_csv('grids_hyperparameters.csv')"
      ],
      "metadata": {
        "id": "aoHOE8jNofi_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}